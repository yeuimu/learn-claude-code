# mini Claude Code v5：让 Agent 永不遗忘 -- 三层上下文压缩

你让 Agent 帮你重构一个大型代码库。它读了 20 个文件、改了 15 个函数、跑了 10 轮测试。然后，在第 60 轮工具调用时，它突然崩溃了——上下文溢出。

这不是偶发事故。v0 到 v4 的所有版本都有一个隐含假设：**对话历史可以无限增长**。现实不是这样。

```sh
200K token 上下文窗口的现实：

  [系统提示词]       ~2K tokens
  [工具定义]          ~8K tokens
  [第 1 轮对话]       +5K tokens
  [第 2 轮对话]       +5K tokens
  ...
  [第 50 轮对话]     已用 ~180K tokens
  [第 60 轮对话]     请求失败: "context_length_exceeded"
```

一个复杂的重构任务可能需要 100 次以上的工具调用。不做任何处理，Agent 一定会撞墙。

v5 用约 100 行新增代码解决了这个问题：**三层上下文压缩**，让 Agent 在有限的窗口里做无限的事。

## 1. 人类记忆的启发

在讲技术细节之前，先想一个问题：你是怎么处理信息过载的？

你在写代码时，不会记住今天写过的每一行。你的大脑自动做了三件事：

1. **短期记忆衰减**：5 分钟前看的函数签名，不用就忘了
2. **概念化**："那个模块做了用户认证相关的事"——细节丢了，概念留了
3. **外部存档**：Git 记录了所有历史，需要时回溯

Agent 的上下文管理需要模拟同样的认知模式。这就是三层压缩的设计灵感。

## 2. 三层压缩模型

```sh
                    Agent 上下文窗口 (200K tokens)
 +------------------------------------------------------------------+
 |  Layer 1: 微压缩 (microcompact)                                    |
 |  触发: 每一轮自动执行                                                |
 |  做法: 把旧的工具输出替换为占位符, 保留最近 3 个                       |
 |  比喻: 短期记忆自动衰减                                              |
 +------------------------------------------------------------------+
 |  Layer 2: 自动压缩 (auto_compact)                                   |
 |  触发: 上下文占用超过 85.3%                                           |
 |  做法: 用模型生成摘要, 替换全部旧消息, 保留最近 5 条                    |
 |  比喻: 细节记忆转为概念记忆                                           |
 +------------------------------------------------------------------+
 |  Layer 3: 手动压缩 (/compact)                                       |
 |  触发: 用户主动输入命令                                               |
 |  做法: 同 Layer 2, 但由用户决定时机                                   |
 |  比喻: "我来整理一下思路"                                             |
 +------------------------------------------------------------------+
 |  磁盘转录 (transcript)                                              |
 |  始终: 完整对话历史保存在 .transcripts/                                |
 |  比喻: 长期记忆归档, 永远不丢                                         |
 +------------------------------------------------------------------+
```

三层不是并列关系，而是**递进关系**。微压缩每轮都做，像呼吸一样自然；自动压缩只在逼近极限时触发，是一次大的记忆整理；手动压缩是用户主动控制的最后手段。

## 3. 核心实现

### 3.1 ContextManager 类

```python
class ContextManager:
    COMPACTABLE_TOOLS = {"bash", "read_file", "Grep", "Glob"}
    KEEP_RECENT = 3
    MAX_OUTPUT_TOKENS = 40000

    def __init__(self, max_context_tokens: int = 200000):
        self.max_context_tokens = max_context_tokens

    @staticmethod
    def estimate_tokens(text: str) -> int:
        """粗略估算: 约 4 个字符 = 1 个 token"""
        return len(text) // 4
```

估算函数是故意"粗糙"的。精确的 tokenizer 调用很慢，对于压缩决策来说，4 字符/token 的近似已经足够。Claude Code 也用类似的快速估算。

### 3.2 Layer 1: 微压缩

```python
def microcompact(self, messages: list) -> list:
    tool_result_indices = []

    for i, msg in enumerate(messages):
        if msg.get("role") != "user":
            continue
        content = msg.get("content")
        if not isinstance(content, list):
            continue
        for j, block in enumerate(content):
            if isinstance(block, dict) and block.get("type") == "tool_result":
                tool_name = self._find_tool_name(messages, block.get("tool_use_id", ""))
                if tool_name in self.COMPACTABLE_TOOLS:
                    tool_result_indices.append((i, j, block))

    to_compact = tool_result_indices[:-self.KEEP_RECENT]

    for i, j, block in to_compact:
        content_str = block.get("content", "")
        if isinstance(content_str, str) and self.estimate_tokens(content_str) > 1000:
            block["content"] = "[Output compacted - re-read if needed]"

    return messages
```

关键设计决策：

- **只替换内容，保留结构**。模型仍然知道"我第 3 轮调用了 bash 执行 ls"，只是看不到输出了。如果需要，它可以重新读取。
- **只清理特定工具**。bash、read_file、Grep、Glob 的输出通常很大。write_file、edit_file 的输出很小（"Wrote 200 bytes"），不值得清理。
- **保留最近 3 个**。最近的工具输出往往是当前工作的上下文，不能丢。

### 3.3 Layer 2: 自动压缩

```python
def auto_compact(self, messages: list) -> list:
    # 1. 先保存完整转录到磁盘
    self.save_transcript(messages)

    # 2. 把所有消息转为纯文本
    conversation_text = self._messages_to_text(messages)

    # 3. 调用模型生成摘要
    summary_response = client.messages.create(
        model=MODEL,
        system="You are a conversation summarizer. Be concise but thorough.",
        messages=[{
            "role": "user",
            "content": f"Summarize this conversation chronologically. "
                       f"Include: goals, actions taken, decisions made, "
                       f"current state, and pending work.\n\n"
                       f"{conversation_text[:100000]}"
        }],
        max_tokens=2000,
    )
    summary = summary_response.content[0].text

    # 4. 摘要 + 最近 5 条消息, 替换全部旧消息
    recent = messages[-5:] if len(messages) > 5 else messages[-2:]
    return [
        {"role": "user", "content": f"[Conversation compressed]\n\n{summary}"},
        {"role": "assistant", "content": "Understood. I have the context. Continuing."},
        *recent
    ]
```

这里有一个极其重要的缓存设计：**摘要注入到对话历史（user message），而不是修改系统提示词**。

```sh
错误做法: 修改 system prompt
  请求 1: [System_v1, User1, Asst1, User2]  <- 全部计算
  请求 2: [System_v2, User1, Asst1, User2]  <- 缓存全部失效, 全部重新计算

正确做法: 追加到消息末尾
  请求 1: [System, User1, Asst1, User2]     <- 全部计算
  请求 2: [System, Summary, Asst, Recent]   <- System 缓存命中
```

### 3.4 大型输出降级

```python
def handle_large_output(self, output: str) -> str:
    if self.estimate_tokens(output) <= self.MAX_OUTPUT_TOKENS:
        return output

    filename = f"output_{int(time.time())}.txt"
    path = TRANSCRIPT_DIR / filename
    path.write_text(output)

    preview = output[:2000]
    return (f"Output too large ({self.estimate_tokens(output)} tokens). "
            f"Saved to: {path}\n\nPreview:\n{preview}...")
```

单次工具输出如果太大（比如 `cat` 一个巨大文件），不等微压缩慢慢清理，直接存盘、只返回预览。模型需要时可以用 read_file 精确读取需要的部分。

## 4. 压缩时序：在 Agent Loop 中的位置

```python
def agent_loop(messages: list) -> list:
    while True:
        # 每轮开始: 先做压缩
        messages = CTX.microcompact(messages)
        if CTX.should_compact(messages):
            print("[Compressing context...]")
            messages = CTX.auto_compact(messages)

        # 然后才调 API
        response = client.messages.create(
            model=MODEL, system=SYSTEM,
            messages=messages, tools=ALL_TOOLS, max_tokens=8000,
        )
        # ... 处理响应、执行工具 ...

        for tc in tool_calls:
            output = execute_tool(tc.name, tc.input)
            output = CTX.handle_large_output(output)  # 工具输出也要检查
```

压缩发生在 API 调用**之前**。这确保发送给模型的上下文永远在窗口限制之内。

子代理同样执行压缩：

```python
def run_task(description, prompt, agent_type):
    sub_messages = [{"role": "user", "content": prompt}]
    while True:
        sub_messages = CTX.microcompact(sub_messages)
        if CTX.should_compact(sub_messages):
            sub_messages = CTX.auto_compact(sub_messages)
        response = client.messages.create(...)
```

## 5. 为什么"遗忘"是一种能力

这是 v5 最深层的洞察。

初看压缩，你可能觉得这是在"丢失信息"。但换个角度想：**如果你能记住过去三年写的每一行代码，你还能专注于当前的 bug 吗？**

人类大脑的工作记忆只有 4-7 个"槽位"。认知科学告诉我们，遗忘不是缺陷，而是一种信息过滤机制——它让你把注意力集中在当前最重要的事情上。

Agent 的上下文窗口也是如此：

| 人类认知 | Agent 压缩 | 本质 |
|----------|-----------|------|
| 短期记忆衰减 | 微压缩 | 自动丢弃低价值细节 |
| 概念化 | 自动压缩 | 细节转为摘要 |
| 笔记/日记 | 磁盘转录 | 外部化长期存储 |
| "需要时查资料" | re-read if needed | 按需恢复 |

压缩不是在"让 Agent 变笨"，而是在模拟人类的注意力管理。完整记录永远在磁盘上（`.transcripts/transcript.jsonl`），压缩只影响**工作记忆**，不影响**存档**。

## 6. 与 Claude Code 的对比

| 机制 | Claude Code | mini Claude Code v5 |
|------|-------------|---------------------|
| 微压缩 | 替换旧 tool_result | 相同逻辑 |
| 自动压缩 | 接近上限时摘要 | 相同逻辑 |
| 手动压缩 | /compact 命令 | 相同 |
| Token 估算 | 精确 tokenizer + 快速估算 | 4 字符/token 近似 |
| 摘要位置 | user message（不动 system） | 相同 |
| 转录存储 | 结构化存储 + Resume 恢复 | JSONL 简单存储 |
| 大输出处理 | 存盘 + 预览 | 相同逻辑 |

核心思路完全一致。省略的部分（Resume 恢复、精确 tokenizer）是工程完善度的差异，不影响理解。

## 7. 九部曲回顾

| 版本 | 核心主题 | 行数 | 关键洞察 |
|------|----------|------|----------|
| v0 | Bash is All | ~50 行 | 一个工具 + 递归 = 完整 Agent |
| v1 | Model as Agent | ~200 行 | 模型是 80%，代码是工具循环 |
| v2 | 结构化规划 | ~300 行 | Todo 工具让计划可见 |
| v3 | 分而治之 | ~450 行 | 子代理隔离上下文 |
| v4 | 领域专家 | ~550 行 | Skills 注入专业知识 |
| **v5** | **永不遗忘** | **~650 行** | **三层压缩模拟人类记忆** |

v5 的新增代码不到 100 行，却解决了一个根本性问题：让 Agent 从"只能做短任务"变成"能做无限长的任务"。

---

**上下文有限，工作无限。遗忘不是缺陷，是让 Agent 永不停歇的能力。**

完整代码见仓库 `v5_compression_agent.py`。
